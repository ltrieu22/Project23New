{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f785fe72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
    "\n",
    "def flatten_conversation(messages):\n",
    "    prompt = \"\"\n",
    "    for m in messages:\n",
    "        if m[\"role\"] == \"user\":\n",
    "            prompt += f\"User: {m['content']}\\n\"\n",
    "        elif m[\"role\"] == \"assistant\":\n",
    "            prompt += f\"Assistant: {m['content']}\\n\"\n",
    "    if not messages or messages[-1][\"role\"] != \"assistant\":\n",
    "        prompt += \"Assistant:\"\n",
    "    return prompt\n",
    "\n",
    "def run_single_turn(input_file, output_file):\n",
    "    with open(input_file, \"r\") as f_in, open(output_file, \"w\") as f_out:\n",
    "        for line in f_in:\n",
    "            example = json.loads(line)\n",
    "            instruction = example[\"instruction\"]\n",
    "            inp = example.get(\"input\", \"\")\n",
    "            prompt = instruction if not inp else f\"{instruction}\\n{inp}\"\n",
    "\n",
    "            generations = generator(\n",
    "                prompt,\n",
    "                max_new_tokens=100,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                repetition_penalty=1.2,\n",
    "                no_repeat_ngram_size=3,\n",
    "                eos_token_id=generator.tokenizer.eos_token_id,\n",
    "                pad_token_id=generator.tokenizer.eos_token_id\n",
    "            )\n",
    "            model_output = generations[0][\"generated_text\"][len(prompt):].strip()\n",
    "\n",
    "            result = {\n",
    "                \"instruction\": instruction,\n",
    "                \"input\": inp,\n",
    "                \"reference_output\": example.get(\"output\", \"\"),\n",
    "                \"model_output\": model_output\n",
    "            }\n",
    "            f_out.write(json.dumps(result) + \"\\n\")\n",
    "\n",
    "def run_multi_turn(input_file, output_file):\n",
    "    with open(input_file, \"r\") as f_in, open(output_file, \"w\") as f_out:\n",
    "        for line in f_in:\n",
    "            example = json.loads(line)\n",
    "            messages = example[\"messages\"]\n",
    "\n",
    "            prompt = flatten_conversation(messages[:-1])\n",
    "\n",
    "            generations = generator(\n",
    "                prompt,\n",
    "                max_new_tokens=100,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                repetition_penalty=1.2,\n",
    "                no_repeat_ngram_size=3, \n",
    "                eos_token_id=generator.tokenizer.eos_token_id,\n",
    "                pad_token_id=generator.tokenizer.eos_token_id\n",
    "            )\n",
    "            model_output = generations[0][\"generated_text\"][len(prompt):].strip()\n",
    "\n",
    "            result = {\n",
    "                \"messages\": messages,\n",
    "                \"reference_output\": messages[-1][\"content\"],\n",
    "                \"model_output\": model_output\n",
    "            }\n",
    "            f_out.write(json.dumps(result) + \"\\n\")\n",
    "\n",
    "run_single_turn(\"single_turn_test.jsonl\", \"baseline_single_turn_outputs.jsonl\")\n",
    "run_multi_turn(\"multi_turn_test.jsonl\", \"baseline_multi_turn_outputs.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "c73167f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets separately...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 80 examples [00:00, 7266.01 examples/s]\n",
      "Generating train split: 80 examples [00:00, 9990.01 examples/s]\n",
      "Generating train split: 10 examples [00:00, 1998.33 examples/s]\n",
      "Generating train split: 10 examples [00:00, 1998.33 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenating datasets...\n",
      "Loaded and combined dataset:\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output', 'constraints', 'evidence_ids', 'messages'],\n",
      "        num_rows: 160\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['instruction', 'input', 'output', 'constraints', 'evidence_ids', 'messages'],\n",
      "        num_rows: 20\n",
      "    })\n",
      "})\n",
      "Tokenizing combined dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 160/160 [00:00<00:00, 1141.81 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 20/20 [00:00<00:00, 832.76 examples/s]\n",
      "C:\\Users\\luant\\AppData\\Local\\Temp\\ipykernel_19608\\3508529908.py:128: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting combined fine-tuning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\luant\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='40' max='40' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [40/40 02:18, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Saving model to ./gpt2-finetuned-combined-final\n",
      "All done. üéâ\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    pipeline, \n",
    "    set_seed, \n",
    "    GPT2Tokenizer, \n",
    "    GPT2LMHeadModel,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from datasets import load_dataset, concatenate_datasets, DatasetDict\n",
    "import torch\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def flatten_conversation(messages):\n",
    "    prompt = \"\"\n",
    "    for m in messages:\n",
    "        if m[\"role\"] == \"user\":\n",
    "            prompt += f\"User: {m['content']}\\n\"\n",
    "        elif m[\"role\"] == \"assistant\":\n",
    "            prompt += f\"Assistant: {m['content']}\\n\"\n",
    "    if not messages or messages[-1][\"role\"] != \"assistant\":\n",
    "        prompt += \"Assistant:\"\n",
    "    return prompt\n",
    "\n",
    "\n",
    "print(\"Loading datasets separately...\")\n",
    "\n",
    "# Note: We use split='train' even for a single file\n",
    "st_train_ds = load_dataset('json', data_files=\"single_turn_train.jsonl\", split=\"train\")\n",
    "mt_train_ds = load_dataset('json', data_files=\"multi_turn_train.jsonl\", split=\"train\")\n",
    "\n",
    "st_val_ds = load_dataset('json', data_files=\"single_turn_val.jsonl\", split=\"train\")\n",
    "mt_val_ds = load_dataset('json', data_files=\"multi_turn_val.jsonl\", split=\"train\")\n",
    "\n",
    "print(\"Concatenating datasets...\")\n",
    "train_ds = concatenate_datasets([st_train_ds, mt_train_ds])\n",
    "validation_ds = concatenate_datasets([st_val_ds, mt_val_ds])\n",
    "\n",
    "datasets = DatasetDict({\n",
    "    'train': train_ds,\n",
    "    'validation': validation_ds\n",
    "})\n",
    "\n",
    "print(f\"Loaded and combined dataset:\")\n",
    "print(datasets)\n",
    "def preprocess_function(examples):\n",
    "    prompts = []\n",
    "    full_texts = []\n",
    "    \n",
    "    for i in range(len(examples['instruction'])):\n",
    "        prompt_part = \"\"\n",
    "        response_content = \"\"\n",
    "\n",
    "        # Case 1: Multi-turn (messages is not None)\n",
    "        if examples['messages'][i] is not None:\n",
    "            messages = examples['messages'][i]\n",
    "            if not messages: continue\n",
    "            prompt_messages = messages[:-1]\n",
    "            response_content = messages[-1]['content']\n",
    "            prompt_part = flatten_conversation(prompt_messages)\n",
    "        \n",
    "        # Case 2: Single-turn (instruction is not None)\n",
    "        elif examples['instruction'][i] is not None:\n",
    "            instruction = examples['instruction'][i]\n",
    "            inp = examples['input'][i]\n",
    "            output = examples['output'][i]\n",
    "            \n",
    "            prompt_content = instruction\n",
    "            if inp:\n",
    "                prompt_content += f\"\\n{inp}\"\n",
    "            prompt_part = f\"User: {prompt_content}\\nAssistant: \"\n",
    "            response_content = output\n",
    "        \n",
    "        else:\n",
    "            continue \n",
    "            \n",
    "        full_text = prompt_part + response_content + tokenizer.eos_token\n",
    "        prompts.append(prompt_part)\n",
    "        full_texts.append(full_text)\n",
    "\n",
    "    model_inputs = tokenizer(\n",
    "        full_texts, max_length=256, truncation=True, padding=\"max_length\"\n",
    "    )\n",
    "    prompt_token_lengths = [\n",
    "        len(tokenizer(p, max_length=256, truncation=True)[\"input_ids\"]) \n",
    "        for p in prompts\n",
    "    ]\n",
    "    labels = [list(row) for row in model_inputs[\"input_ids\"]]\n",
    "    for i in range(len(labels)):\n",
    "        prompt_len = prompt_token_lengths[i]\n",
    "        actual_prompt_len = min(prompt_len, len(labels[i]))\n",
    "        labels[i][:actual_prompt_len] = [-100] * actual_prompt_len\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels\n",
    "    return model_inputs\n",
    "\n",
    "print(\"Tokenizing combined dataset...\")\n",
    "tokenized_datasets = datasets.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=datasets[\"train\"].column_names \n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2-finetuned-combined\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=1.0,                   \n",
    "    per_device_train_batch_size=4,          \n",
    "    per_device_eval_batch_size=4,\n",
    "    learning_rate=5e-5,                     \n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs-combined',\n",
    "    logging_steps=100,                      \n",
    "    eval_strategy=\"steps\",                  \n",
    "    eval_steps=200,\n",
    "    save_steps=200,                         \n",
    "    save_total_limit=2,                     \n",
    "    fp16=torch.cuda.is_available(),         \n",
    "    report_to=\"none\"  \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"Starting combined fine-tuning...\")\n",
    "trainer.train()\n",
    "\n",
    "final_model_path = \"./gpt2-finetuned-combined-final\"\n",
    "print(f\"Training complete. Saving model to {final_model_path}\")\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "\n",
    "print(\"All done. üéâ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec25795",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fine-tuned model from: ./gpt2-finetuned-combined-final\n",
      "Model loaded.\n",
      "Running single-turn inference on single_turn_test.jsonl...\n",
      "Single-turn results saved to combined_finetuned_single_turn_outputs.jsonl\n",
      "Running multi-turn inference on multi_turn_test.jsonl...\n",
      "Multi-turn results saved to combined_finetuned_multi_turn_outputs.jsonl\n",
      "\n",
      "All fine-tuned (combined model) inference complete. ‚úÖ\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline, set_seed, GPT2Tokenizer\n",
    "\n",
    "FINETUNED_MODEL_PATH = \"./gpt2-finetuned-combined-final\" \n",
    "\n",
    "print(f\"Loading fine-tuned model from: {FINETUNED_MODEL_PATH}\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(FINETUNED_MODEL_PATH)\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=FINETUNED_MODEL_PATH,\n",
    "    tokenizer=tokenizer,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "def flatten_conversation(messages):\n",
    "    prompt = \"\"\n",
    "    for m in messages:\n",
    "        if m[\"role\"] == \"user\":\n",
    "            prompt += f\"User: {m['content']}\\n\"\n",
    "        elif m[\"role\"] == \"assistant\":\n",
    "            prompt += f\"Assistant: {m['content']}\\n\"\n",
    "    if not messages or messages[-1][\"role\"] != \"assistant\":\n",
    "        prompt += \"Assistant:\"\n",
    "    return prompt\n",
    "\n",
    "def run_single_turn_finetuned(input_file, output_file):\n",
    "    print(f\"Running single-turn inference on {input_file}...\")\n",
    "    with open(input_file, \"r\") as f_in, open(output_file, \"w\") as f_out:\n",
    "        for line in f_in:\n",
    "            example = json.loads(line)\n",
    "            instruction = example[\"instruction\"]\n",
    "            inp = example.get(\"input\", \"\")\n",
    "            \n",
    "            prompt_content = instruction if not inp else f\"{instruction}\\n{inp}\"\n",
    "            prompt = f\"User: {prompt_content}\\nAssistant:\"\n",
    "\n",
    "            generations = generator(\n",
    "                prompt,\n",
    "                max_new_tokens=100,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                repetition_penalty=1.2,\n",
    "                no_repeat_ngram_size=3,\n",
    "                eos_token_id=tokenizer.eos_token_id, \n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            \n",
    "            model_output = generations[0][\"generated_text\"][len(prompt):].strip()\n",
    "\n",
    "            result = {\n",
    "                \"instruction\": instruction,\n",
    "                \"input\": inp,\n",
    "                \"reference_output\": example.get(\"output\", \"\"),\n",
    "                \"model_output\": model_output\n",
    "            }\n",
    "            f_out.write(json.dumps(result) + \"\\n\")\n",
    "    print(f\"Single-turn results saved to {output_file}\")\n",
    "\n",
    "def run_multi_turn_finetuned(input_file, output_file):\n",
    "    print(f\"Running multi-turn inference on {input_file}...\")\n",
    "    with open(input_file, \"r\") as f_in, open(output_file, \"w\") as f_out:\n",
    "        for line in f_in:\n",
    "            example = json.loads(line)\n",
    "            messages = example[\"messages\"]\n",
    "            prompt = flatten_conversation(messages[:-1])\n",
    "\n",
    "            generations = generator(\n",
    "                prompt,\n",
    "                max_new_tokens=100,\n",
    "                num_return_sequences=1,\n",
    "                do_sample=True,\n",
    "                repetition_penalty=1.2,\n",
    "                no_repeat_ngram_size=3,\n",
    "                eos_token_id=tokenizer.eos_token_id,\n",
    "                pad_token_id=tokenizer.eos_token_id\n",
    "            )\n",
    "            model_output = generations[0][\"generated_text\"][len(prompt):].strip()\n",
    "\n",
    "            result = {\n",
    "                \"messages\": messages,\n",
    "                \"reference_output\": messages[-1][\"content\"],\n",
    "                \"model_output\": model_output\n",
    "            }\n",
    "            f_out.write(json.dumps(result) + \"\\n\")\n",
    "    print(f\"Multi-turn results saved to {output_file}\")\n",
    "\n",
    "run_single_turn_finetuned(\n",
    "    \"single_turn_test.jsonl\", \n",
    "    \"combined_finetuned_single_turn_outputs.jsonl\"\n",
    ")\n",
    "\n",
    "run_multi_turn_finetuned(\n",
    "    \"multi_turn_test.jsonl\", \n",
    "    \"combined_finetuned_multi_turn_outputs.jsonl\"\n",
    ")\n",
    "\n",
    "print(\"\\nAll fine-tuned (combined model) inference complete. ‚úÖ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f9a4dfbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " DETAILED SINGLE-TURN COMPARISON\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_0fe7f th {\n",
       "  text-align: center;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_0fe7f td {\n",
       "  padding: 10px;\n",
       "  border: 1px solid #ddd;\n",
       "}\n",
       "#T_0fe7f_row0_col0, #T_0fe7f_row0_col1, #T_0fe7f_row0_col2, #T_0fe7f_row0_col3, #T_0fe7f_row0_col4, #T_0fe7f_row1_col0, #T_0fe7f_row1_col1, #T_0fe7f_row1_col2, #T_0fe7f_row1_col3, #T_0fe7f_row1_col4, #T_0fe7f_row2_col0, #T_0fe7f_row2_col1, #T_0fe7f_row2_col2, #T_0fe7f_row2_col3, #T_0fe7f_row2_col4, #T_0fe7f_row3_col0, #T_0fe7f_row3_col1, #T_0fe7f_row3_col2, #T_0fe7f_row3_col3, #T_0fe7f_row3_col4, #T_0fe7f_row4_col0, #T_0fe7f_row4_col1, #T_0fe7f_row4_col2, #T_0fe7f_row4_col3, #T_0fe7f_row4_col4 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 300px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_0fe7f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_0fe7f_level0_col0\" class=\"col_heading level0 col0\" >Example</th>\n",
       "      <th id=\"T_0fe7f_level0_col1\" class=\"col_heading level0 col1\" >Instruction</th>\n",
       "      <th id=\"T_0fe7f_level0_col2\" class=\"col_heading level0 col2\" >Reference</th>\n",
       "      <th id=\"T_0fe7f_level0_col3\" class=\"col_heading level0 col3\" >Baseline Output</th>\n",
       "      <th id=\"T_0fe7f_level0_col4\" class=\"col_heading level0 col4\" >Fine-tuned Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_0fe7f_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_0fe7f_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_0fe7f_row0_col1\" class=\"data row0 col1\" >Find four highly-rated gluten-free recipes with at least 5.0 stars and under 20 minutes.</td>\n",
       "      <td id=\"T_0fe7f_row0_col2\" class=\"data row0 col2\" >1) Cottage-Cheese Stuffed Tomatoes√¢‚Ç¨‚Äù10 min; 2) Nutritious Food Enhancer√¢‚Ç¨‚Äù15 min; 3) Chili Lime BBQ Mangoes√¢‚Ç¨‚Äù15 min; 4) No-Bake Low-Carb Protein Bars√¢‚Ç¨‚Äù5 min</td>\n",
       "      <td id=\"T_0fe7f_row0_col3\" class=\"data row0 col3\" ></td>\n",
       "      <td id=\"T_0fe7f_row0_col4\" class=\"data row0 col4\" >(1) Spicy Peanut Butter Banana Bread‚Äî7 min; 2), Gluten Free, Stuffed Rice & Garlic Oatmeal Cookies Made Easy With Scallion Muffins ‚Äî2 1/4 hr 25mins; 3)‚ÄîChocolate Cucumber Cookie Doughnuts And Creamy Baked Sweet Potato Soup ‚òÖ‚òÖ‚òÖ‚òÖ 4 g carbs ‚Ä¢ Serving Size : 30g protein ‚â•30 mg carbohydrates - including low sodium); 4.) Low Carb Chocolate Chip Brownies! #253529 |</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0fe7f_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_0fe7f_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_0fe7f_row1_col1\" class=\"data row1 col1\" >Find four high-protein lunchs over 34g protein in under 35 minutes.</td>\n",
       "      <td id=\"T_0fe7f_row1_col2\" class=\"data row1 col2\" >1) Awesome Steamed Cheeseburgers!√¢‚Ç¨‚Äù46.0 g protein√¢‚Ç¨‚Äù16 min; 2) Vegetarian Sandwich with Herbed Cream Cheese and Guacamole√¢‚Ç¨‚Äù40.5 g protein√¢‚Ç¨‚Äù10 min; 3) Barbecued Buffalo Wing Dip With a Twist√¢‚Ç¨‚Äù195.9 g protein√¢‚Ç¨‚Äù25 min; 4) Aussie Tuna Summer Salad√¢‚Ç¨‚Äù47.4 g protein√¢‚Ç¨‚Äù16 min</td>\n",
       "      <td id=\"T_0fe7f_row1_col3\" class=\"data row1 col3\" >The other two meals are very low on carbs and fat, so the higher calorie versions will be your best bet for keeping you lean throughout this whole process! No dairy or gluten products included - just a good amount of plain flour & sugar (which is what we've been using). This works great as long it's not too hard to make enough extra filling out each day while eating breakfast cereal with some fruit juice etc... It also helps keep all dry ingredients down from being added later when there</td>\n",
       "      <td id=\"T_0fe7f_row1_col4\" class=\"data row1 col4\" >1) Chunky Peanut Butter Chicken Soup‚Äî35 min; 2). Baked French Toast ‚Äî45 min (no added sugar), served with a side of broccoli and tomatoes, bacon zest ($14)‚Äî30 min.; 3.) Kale Salad With Bacon Cheddar Cheese Sauce on the Side‚Äî10 min/d.‚Äî42 g carbs; 4] Eggnog Burger at Home Crab Steakhouse Chili Tacos & Tenders Pizza Pizzeria's BBQ Pork Pot Pie Sandwich\"‚Äî25 h</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0fe7f_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_0fe7f_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_0fe7f_row2_col1\" class=\"data row2 col1\" >Find four low-carb dinners under 12g total carbohydrates with at least 34g protein.</td>\n",
       "      <td id=\"T_0fe7f_row2_col2\" class=\"data row2 col2\" >1) Egyptian Red Snapper in Red Pepper Mint Sauce√¢‚Ç¨‚Äù60.1 g protein; 2) Mustard-Grilled Scandinavian Salmon√¢‚Ç¨‚Äù34.9 g protein; 3) Moroccan-Spiced Tuna√¢‚Ç¨‚Äù53.4 g protein; 4) Melodee's Hot Wings√¢‚Ç¨‚Äù207.7 g protein</td>\n",
       "      <td id=\"T_0fe7f_row2_col3\" class=\"data row2 col3\" >This is the recipe for a pretty simple keto meal that you can make after your workout and still use in all of those workouts if needs be! I'm sure most people will have noticed, but there are other ingredients available which we'll discuss later on this post: 8 oz cans chickpeas or 1 cup canned pumpkin pie (I recommend using 2 cups) 30 g water 24 G sugar 16% fat 6 grams carbohydrate One serving x 14 servings Carbohydrates = 17 calories Serving Size Calories</td>\n",
       "      <td id=\"T_0fe7f_row2_col4\" class=\"data row2 col4\" >1) Low Carb Chicken and Cheese‚Äî85 g carbs; 2). Glazed Cauliflower Soup (Spaghetti Squash)‚Äî56 mg carb; 3); Dessert Crusty Sweet & Sour Cream Pie With Honey Cheesecake Sauce in a Cupcake Mix‚Äî43 kcal, plus 13 min.; 4.) Apple Noodle Bowls Under 30 G Protein\"‚Äî15 wk glycemic control or less; 5.). Chocolate Peanut Butter Rimmel Icecream Sandwich Bars Over 50‚Äì60 mL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0fe7f_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_0fe7f_row3_col0\" class=\"data row3 col0\" >4</td>\n",
       "      <td id=\"T_0fe7f_row3_col1\" class=\"data row3 col1\" >Find four highly-rated vegan recipes with at least 5.0 stars and under 30 minutes.</td>\n",
       "      <td id=\"T_0fe7f_row3_col2\" class=\"data row3 col2\" >1) Tomato Lentil Stew√¢‚Ç¨‚Äù25 min; 2) Vegan Waldorf Salad√¢‚Ç¨‚Äù20 min; 3) Quick Elephant Ears√¢‚Ç¨‚Äù10 min; 4) Summer Fruit & Vegetable Medley√¢‚Ç¨‚Äù18 min</td>\n",
       "      <td id=\"T_0fe7f_row3_col3\" class=\"data row3 col3\" >10) Chocolate Chip Cookies (Strawberry, Cream Cheese & Jelly), Vegan Eggplant Roll or Cookie Dough Tossed With Peanut Butter M&Ms ‚Äì 4 Ingredients: 6 ounces of whole wheat flour 2 tablespoons coconut oil 1/2 teaspoon salt 3 teaspoons baking soda ¬º cup almond milk ¬Ω tablespoon sugar Total Prep time 40 mins Cooking Time 20 min Preheat oven to 350 degrees F / 200¬∞C Spray a 9√ó9 pan over medium heat; place all ingredients in the pot</td>\n",
       "      <td id=\"T_0fe7f_row3_col4\" class=\"data row3 col4\" >1) Tomato Lime Pudding‚Äî6 min; 2!) Red Pepper Chicken Breast With Creamy Garlic Butter Sauce (Frozen)‚Äî4 min, Serves 3‚Äì8), Grilled Cheese Pot Pie & Desserts 434 mg sodium ‚Äî1 g carbs ‚Ä¢2.) Green Salad Mashed Potato Soup in Arugula Nuts\"‚Äî5 hrs.;}3.‚ÄîGrilling Salmon Steak Bowl on Peppers' Sesame Seeds‚Äî\"13 cms\";]Daily Menu Items\" Daily</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_0fe7f_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_0fe7f_row4_col0\" class=\"data row4 col0\" >5</td>\n",
       "      <td id=\"T_0fe7f_row4_col1\" class=\"data row4 col1\" >Find four highly-rated vegetarian recipes with at least 4.0 stars and under 30 minutes.</td>\n",
       "      <td id=\"T_0fe7f_row4_col2\" class=\"data row4 col2\" >1) Super-Quick Brown Rice With Asparagus, Chickpeas, and Almonds√¢‚Ç¨‚Äù15 min; 2) The Best Cranberry Chutney√¢‚Ç¨‚Äù15 min; 3) Lychees & Ice Cream (With and Without Frangelico or Amaretto√¢‚Ç¨‚Äù5 min; 4) New-Fashioned Apple and Raisin Slaw (From Cooking Light)√¢‚Ç¨‚Äù10 min</td>\n",
       "      <td id=\"T_0fe7f_row4_col3\" class=\"data row4 col3\" >I'm a bit of an inane eater myself, so I made this recipe for my first time on the blog (just to let you know it's not that bad). It is super simple but very fast! Also make sure to add some salt if using too much water as most people will find salty when eating green beans or carrots which can cause them to get stuck after they've been eaten since day 1!! üôÇ Here are three other great vegan options:\n",
       "\n",
       "1) Potatoes ‚Äì This</td>\n",
       "      <td id=\"T_0fe7f_row4_col4\" class=\"data row4 col4\" >1.) Cauliflower With Balsamic Roast Beef‚Äî3 min; 2) Chicken Potatoes Tofu Soup (Cheesy)‚Äî23 g protein, 11g carbohydrates; 3). Ginger Basil Chili Peppers ($14), Lightly Spicy ‚Äî4 hr 35 mins; 5)(a) Carrot & Garlic Cream Pie Salad.‚Äî25 mL carbs, 17 mg sodium; 6)) Pesto Desserts Pastry Sauce in Spanish Style Pizza Bars‚Äî1 h 39 sec;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x187a2b3ddd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      " DETAILED MULTI-TURN COMPARISON\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_150a7 th {\n",
       "  text-align: center;\n",
       "  font-weight: bold;\n",
       "}\n",
       "#T_150a7 td {\n",
       "  padding: 10px;\n",
       "  border: 1px solid #ddd;\n",
       "}\n",
       "#T_150a7_row0_col0, #T_150a7_row0_col1, #T_150a7_row0_col2, #T_150a7_row0_col3, #T_150a7_row0_col4, #T_150a7_row1_col0, #T_150a7_row1_col1, #T_150a7_row1_col2, #T_150a7_row1_col3, #T_150a7_row1_col4, #T_150a7_row2_col0, #T_150a7_row2_col1, #T_150a7_row2_col2, #T_150a7_row2_col3, #T_150a7_row2_col4, #T_150a7_row3_col0, #T_150a7_row3_col1, #T_150a7_row3_col2, #T_150a7_row3_col3, #T_150a7_row3_col4, #T_150a7_row4_col0, #T_150a7_row4_col1, #T_150a7_row4_col2, #T_150a7_row4_col3, #T_150a7_row4_col4 {\n",
       "  text-align: left;\n",
       "  white-space: pre-wrap;\n",
       "  word-wrap: break-word;\n",
       "  max-width: 300px;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_150a7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_150a7_level0_col0\" class=\"col_heading level0 col0\" >Example</th>\n",
       "      <th id=\"T_150a7_level0_col1\" class=\"col_heading level0 col1\" >Instruction</th>\n",
       "      <th id=\"T_150a7_level0_col2\" class=\"col_heading level0 col2\" >Reference</th>\n",
       "      <th id=\"T_150a7_level0_col3\" class=\"col_heading level0 col3\" >Baseline Output</th>\n",
       "      <th id=\"T_150a7_level0_col4\" class=\"col_heading level0 col4\" >Fine-tuned Output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_150a7_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_150a7_row0_col0\" class=\"data row0 col0\" >1</td>\n",
       "      <td id=\"T_150a7_row0_col1\" class=\"data row0 col1\" >user: I need lunch ideas.\n",
       "assistant: What's your time constraint and protein goal?\n",
       "user: Under 55 minutes, at least 17g.</td>\n",
       "      <td id=\"T_150a7_row0_col2\" class=\"data row0 col2\" >1) Crispy Chicken Fingers with Spicy Honey Mustard√¢‚Ç¨‚Äù62.8 g protein√¢‚Ç¨‚Äù25 min; 2) California Avocado Monte Cristo√¢‚Ç¨‚Äù22.5 g protein√¢‚Ç¨‚Äù18 min; 3) Chicken Burrito Bundles√¢‚Ç¨‚Äù33.4 g protein√¢‚Ç¨‚Äù30 min; 4) Meatloaf Sandwiches Without the Loaf√¢‚Ç¨‚Äù34.2 g protein√¢‚Ç¨‚Äù40 min</td>\n",
       "      <td id=\"T_150a7_row0_col3\" class=\"data row0 col3\" >How long do you want to be away from work for about 5 months before joining the company because of this job requirement?? Thanks For reading! Reply Delete</td>\n",
       "      <td id=\"T_150a7_row0_col4\" class=\"data row0 col4\" >1) Spaghetti; 2‚Äî15 min ‚Äî2 g carbs (from 1 egg); 3)‚Äî20 Min ‚Äî26 mg sodium; 4‚Äì6 wk lactose free electrolytes available on site in 24-mL increments or under 32 mL.* This low carb version includes no added sugars except for the high potassium content of sorrels which can increase from 15 to 25 mmol/L if you have trouble digesting it.)\n",
       "Customer Service Representative: Are there any dietary restrictions with this program that</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_150a7_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_150a7_row1_col0\" class=\"data row1 col0\" >2</td>\n",
       "      <td id=\"T_150a7_row1_col1\" class=\"data row1 col1\" >user: Find lunch ideas.\n",
       "assistant: Are you looking for low-carb options?\n",
       "user: Yes √¢‚Ç¨‚Äù under 12g carbs and at least 25g protein.</td>\n",
       "      <td id=\"T_150a7_row1_col2\" class=\"data row1 col2\" >1) Crunchy Tuna Walnut Salad√¢‚Ç¨‚Äù25.0 g protein√¢‚Ç¨‚Äù4.9 g carbs; 2) Whole Foods' Tarragon Chicken Salad√¢‚Ç¨‚Äù26.1 g protein√¢‚Ç¨‚Äù8.0 g carbs; 3) Hackney's Inside-Out Burger√¢‚Ç¨‚Äù59.7 g protein√¢‚Ç¨‚Äù0.8 g carbs; 4) Appetizer or Snack Party Tray√¢‚Ç¨‚Äù27.7 g protein√¢‚Ç¨‚Äù7.2 g carbs</td>\n",
       "      <td id=\"T_150a7_row1_col3\" class=\"data row1 col3\" >What are your ketogenic diet plans, especially if they include the carb restriction that is suggested by Dr Gossett in his book \"Fat Loss\"? Do you have a plan to add weight or not limit carbohydrates during this period of time (eg when starting out)? Should we focus on calorie burn rather than caloric intake only as nutritionist advice while staying within our target range based upon nutritional facts about what works best with us/the body's needs etc? Is there any other option available which would be</td>\n",
       "      <td id=\"T_150a7_row1_col4\" class=\"data row1 col4\" >1) Low Carb Baked Chicken Salad with Lemon Dressing‚Äî40 min; 2)‚ÄîChicken Fried With Grilled Cucumber Sauce (12 g carbohydrates); 3).5 mg sodium, 55 minutes; 4), Cornbread Omelet of Summer Rice Soup & Pasta by Olga Koczorowicz‚ÄìDiazma in Italian Style or Sweet Potato Salsa Made Easy By Using A Vegan Tomato Paste! 1.) Carrot Pesto Quesadillas Deli Choco's</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_150a7_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_150a7_row2_col0\" class=\"data row2 col0\" >3</td>\n",
       "      <td id=\"T_150a7_row2_col1\" class=\"data row2 col1\" >user: I need breakfast ideas.\n",
       "assistant: What's your time constraint and carbs goal?\n",
       "user: Under 55 minutes, under 24g.</td>\n",
       "      <td id=\"T_150a7_row2_col2\" class=\"data row2 col2\" >1) Campbelled Eggs√¢‚Ç¨‚Äù7.1 g carbs√¢‚Ç¨‚Äù10 min; 2) Asparagus, Mushroom and Cheese Omelet With Herbs√¢‚Ç¨‚Äù3.7 g carbs√¢‚Ç¨‚Äù15 min; 3) Corsican Omelette√¢‚Ç¨‚Äù4.0 g carbs√¢‚Ç¨‚Äù6 min; 4) Vegan Rice Pudding√¢‚Ç¨‚Äù11.6 g carbs√¢‚Ç¨‚Äù15 min</td>\n",
       "      <td id=\"T_150a7_row2_col3\" class=\"data row2 col3\" >So why are you doing this over the past 3 days to get rid of all that carb consumption then?? You guys know what it does when we're not making a big commitment about food intake! It takes some getting used into conscious thought because if too much protein is absorbed or there was no appetite for carbohydrate at just 6 weeks old (or maybe even later?) those calories would be depleted as well...just like our children who will see their body take in less fat than they already have on average -</td>\n",
       "      <td id=\"T_150a7_row2_col4\" class=\"data row2 col4\" >1) Broccoli‚Äî35 min; 2). Smoked Salmon ‚Äî85 mL Fritos (30 g); 3)‚ÄîCucumber Salad with Bacon & Cream Sauce on the Pineapple Side.‚Äî20 h.; 4.) Peanut Butter Lemonade Sandwich at Home With Lime-Baked Cheese Balls in Tomato Basil Soup [MONDAYS]; 5)) Cajun Chicken Parmesan Pasta Cookies by Julia Childeau Style! Arugula Roasted Turkey Breast Wings Prepared By Lisa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_150a7_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_150a7_row3_col0\" class=\"data row3 col0\" >4</td>\n",
       "      <td id=\"T_150a7_row3_col1\" class=\"data row3 col1\" >user: I want to make soup.\n",
       "assistant: Any dietary restrictions or sodium concerns?\n",
       "user: Yes, low sodium under 576 mg and vegetarian.</td>\n",
       "      <td id=\"T_150a7_row3_col2\" class=\"data row3 col2\" >1) Zesty Mexican Tomato Soup√¢‚Ç¨‚Äù519.1 mg sodium; 2) Garlic Stock for Soups√¢‚Ç¨‚Äù129.2 mg sodium; 3) Lebanese Adas Bis Silq (Lentil & Swisschard Soup)√¢‚Ç¨‚Äù218.4 mg sodium; 4) Moroccan Harira Soup√¢‚Ç¨‚Äù182.0 mg sodium</td>\n",
       "      <td id=\"T_150a7_row3_col3\" class=\"data row3 col3\" >Anything else we can do for you this week that's just a little bit more flexible with what is going on in your diet right now..and how much of it will be consumed during the day/night cycle while avoiding caffeine (aka \"dieting\") etc.? Is there any difference between consuming fruits at night instead when eating them as opposed from their normal schedule? Or are all meals allowed if they're cooked up earlier than usual, even though most people don't have access towards half-hour</td>\n",
       "      <td id=\"T_150a7_row3_col4\" class=\"data row3 col4\" >1) Chicken Soup‚Äî15 min; 2)(i.) Lemon Ginger Salad With Carrots & Cucumber Dressing (Low Sodium)‚Äî23 g carbs; 3)) Chalk-O'Ran Risotto with Garlic Basil Roast Beef ‚Äî25 mL protein); 4), Salmon Nachos at the Beach Saloon Grill in Los Angeles County Park for $10/2 lb.‚Äî3%) Vegetarian Pork Sausage Dinner ($30). Assistant(s): 1)* Eggplant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_150a7_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_150a7_row4_col0\" class=\"data row4 col0\" >5</td>\n",
       "      <td id=\"T_150a7_row4_col1\" class=\"data row4 col1\" >user: Find lunch ideas.\n",
       "assistant: Are you looking for low-carb options?\n",
       "user: Yes √¢‚Ç¨‚Äù under 18g carbs and at least 34g protein.</td>\n",
       "      <td id=\"T_150a7_row4_col2\" class=\"data row4 col2\" >1) Grilled Blue Cheese Burgers√¢‚Ç¨‚Äù37.1 g protein√¢‚Ç¨‚Äù2.1 g carbs; 2) Barbecued Lobster Tails√¢‚Ç¨‚Äù58.6 g protein√¢‚Ç¨‚Äù8.3 g carbs; 3) Sandra Lee's Beef Kebabs√¢‚Ç¨‚Äù48.1 g protein√¢‚Ç¨‚Äù4.0 g carbs; 4) Any-Way-You-Want 'em Burgers√¢‚Ç¨‚Äù36.5 g protein√¢‚Ç¨‚Äù7.5 g carbs</td>\n",
       "      <td id=\"T_150a7_row4_col3\" class=\"data row4 col3\" >Do people want to get ketosis from eating lean meat or do they prefer fat/cholesterol intake based on their body weight? Is there any other specific dietary goals that can be achieved without dieting too much as my clients have mentioned, like the need of a good cardio program while also making sure food is healthy all year round (which I don't see with many carb intakes)? It's an important question but we'll just keep it simple here because this will help us better understand how our goal</td>\n",
       "      <td id=\"T_150a7_row4_col4\" class=\"data row4 col4\" >1) Chicken Soup‚Äî25 min; 2). Pinch Of Red Pepper Cauliflower Salad Sandwich, Fried With Cornstarch & Garlic Parmesan Cheese Cream Sauce ‚Äî32 g carb; 3)‚ÄîCucumber Grilled Fish (Pumpkin Stuffed Salmon), 4). Fresh Spinach Spiced Tuna Tacos with Tomato Juice Lime Potatoes And Lemon Nectarines‚Äî28 mg carb.; 5); Honey Butter Macaroni Salsa Dip‚Äî44 ml carbohydrate; 6)(Note</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x187a2b3d2d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "def compare_outputs(baseline_file, finetuned_file, num_examples=10):\n",
    "    \"\"\"\n",
    "    Display side-by-side comparison of baseline vs fine-tuned outputs\n",
    "    \"\"\"\n",
    "    baseline_data = []\n",
    "    finetuned_data = []\n",
    "    \n",
    "    # Load baseline outputs\n",
    "    with open(baseline_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            baseline_data.append(json.loads(line))\n",
    "    \n",
    "    # Load fine-tuned outputs\n",
    "    with open(finetuned_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            finetuned_data.append(json.loads(line))\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparisons = []\n",
    "    for i in range(min(num_examples, len(baseline_data), len(finetuned_data))):\n",
    "        baseline = baseline_data[i]\n",
    "        finetuned = finetuned_data[i]\n",
    "        \n",
    "        # Handle single-turn format\n",
    "        if \"instruction\" in baseline:\n",
    "            instruction = baseline[\"instruction\"]\n",
    "            if baseline.get(\"input\"):\n",
    "                instruction += f\"\\nInput: {baseline['input']}\"\n",
    "        # Handle multi-turn format\n",
    "        else:\n",
    "            messages = baseline[\"messages\"]\n",
    "            instruction = \"\\n\".join([f\"{m['role']}: {m['content']}\" for m in messages[:-1]])\n",
    "        \n",
    "        comparisons.append({\n",
    "            \"Example\": i + 1,\n",
    "            \"Instruction\": instruction,\n",
    "            \"Reference\": baseline[\"reference_output\"],\n",
    "            \"Baseline Output\": baseline[\"model_output\"],\n",
    "            \"Fine-tuned Output\": finetuned[\"model_output\"]\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(comparisons)\n",
    "    \n",
    "    # Style the dataframe for better readability\n",
    "    styled_df = df.style.set_properties(**{\n",
    "        'text-align': 'left',\n",
    "        'white-space': 'pre-wrap',\n",
    "        'word-wrap': 'break-word',\n",
    "        'max-width': '300px'\n",
    "    }).set_table_styles([\n",
    "        {'selector': 'th', 'props': [('text-align', 'center'), ('font-weight', 'bold')]},\n",
    "        {'selector': 'td', 'props': [('padding', '10px'), ('border', '1px solid #ddd')]}\n",
    "    ])\n",
    "    \n",
    "    return styled_df\n",
    "\n",
    "def compare_outputs_simple(baseline_file, finetuned_file, num_examples=10):\n",
    "    \"\"\"\n",
    "    Simple text-based comparison (no pandas styling)\n",
    "    \"\"\"\n",
    "    baseline_data = []\n",
    "    finetuned_data = []\n",
    "    \n",
    "    with open(baseline_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            baseline_data.append(json.loads(line))\n",
    "    \n",
    "    with open(finetuned_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            finetuned_data.append(json.loads(line))\n",
    "    \n",
    "    print(\"=\" * 120)\n",
    "    print(f\"{'INSTRUCTION':<40} | {'BASELINE OUTPUT':<40} | {'FINE-TUNED OUTPUT':<40}\")\n",
    "    print(\"=\" * 120)\n",
    "    \n",
    "    for i in range(min(num_examples, len(baseline_data), len(finetuned_data))):\n",
    "        baseline = baseline_data[i]\n",
    "        finetuned = finetuned_data[i]\n",
    "        \n",
    "        # Get instruction\n",
    "        if \"instruction\" in baseline:\n",
    "            instruction = baseline[\"instruction\"]\n",
    "            if baseline.get(\"input\"):\n",
    "                instruction += f\" | {baseline['input']}\"\n",
    "        else:\n",
    "            messages = baseline[\"messages\"]\n",
    "            instruction = messages[-2][\"content\"] if len(messages) > 1 else \"Multi-turn\"\n",
    "        \n",
    "        # Truncate long texts\n",
    "        instruction = (instruction[:37] + \"...\") if len(instruction) > 40 else instruction\n",
    "        baseline_out = (baseline[\"model_output\"][:37] + \"...\") if len(baseline[\"model_output\"]) > 40 else baseline[\"model_output\"]\n",
    "        finetuned_out = (finetuned[\"model_output\"][:37] + \"...\") if len(finetuned[\"model_output\"]) > 40 else finetuned[\"model_output\"]\n",
    "        \n",
    "        print(f\"\\n{i+1}. {instruction:<38} | {baseline_out:<38} | {finetuned_out:<38}\")\n",
    "        print(\"-\" * 120)\n",
    "\n",
    "\n",
    "# If you want fancy pandas table (better for notebooks):\n",
    "print(\"\\n\\n DETAILED SINGLE-TURN COMPARISON\\n\")\n",
    "display(compare_outputs(\n",
    "    \"baseline_single_turn_outputs.jsonl\",\n",
    "    \"combined_finetuned_single_turn_outputs.jsonl\",\n",
    "    num_examples=5\n",
    "))\n",
    "\n",
    "print(\"\\n\\n DETAILED MULTI-TURN COMPARISON\\n\")\n",
    "display(compare_outputs(\n",
    "    \"baseline_multi_turn_outputs.jsonl\",\n",
    "    \"combined_finetuned_multi_turn_outputs.jsonl\",\n",
    "    num_examples=5\n",
    "))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
